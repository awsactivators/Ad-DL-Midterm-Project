{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0a9ceb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'fashionmnist_model' from '/Users/noah/Humber/Advanced-DL/Ad-DL-Midterm-Project/fashionmnist_model.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import fashionmnist_model as fmm_module  # Ensures clarity by matching the file name\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, Adagrad\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reloads the module to ensure any changes are applied\n",
    "importlib.reload(fmm_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c588bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = fmm_module.FMM.load_data()\n",
    "X_train, X_test = fmm_module.FMM.reshape_data(X_train, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eed54881",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = {\n",
    "    'RMSprop': RMSprop,\n",
    "    'Adam': Adam,\n",
    "    'Adagrad': Adagrad\n",
    "}\n",
    "learning_rates = [0.001, 0.0005, 0.0001]\n",
    "batch_sizes = [32, 64, 128]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b29b636c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with RMSprop, LR: 0.001, Batch Size: 32\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 09:11:21.662460: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 3s 1ms/step - loss: 0.5178 - accuracy: 0.8123 - val_loss: 0.4764 - val_accuracy: 0.8253\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3888 - accuracy: 0.8599 - val_loss: 0.3789 - val_accuracy: 0.8649\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3653 - accuracy: 0.8697 - val_loss: 0.4546 - val_accuracy: 0.8461\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.3497 - accuracy: 0.8768 - val_loss: 0.3941 - val_accuracy: 0.8667\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3399 - accuracy: 0.8800 - val_loss: 0.3820 - val_accuracy: 0.8702\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3291 - accuracy: 0.8854 - val_loss: 0.3942 - val_accuracy: 0.8822\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3256 - accuracy: 0.8875 - val_loss: 0.4119 - val_accuracy: 0.8647\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3186 - accuracy: 0.8905 - val_loss: 0.4124 - val_accuracy: 0.8709\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3131 - accuracy: 0.8933 - val_loss: 0.4412 - val_accuracy: 0.8752\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3073 - accuracy: 0.8947 - val_loss: 0.4275 - val_accuracy: 0.8745\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3003 - accuracy: 0.8978 - val_loss: 0.5462 - val_accuracy: 0.8704\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3039 - accuracy: 0.8977 - val_loss: 0.4604 - val_accuracy: 0.8717\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2940 - accuracy: 0.9021 - val_loss: 0.4910 - val_accuracy: 0.8720\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2923 - accuracy: 0.9015 - val_loss: 0.5142 - val_accuracy: 0.8729\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2865 - accuracy: 0.9043 - val_loss: 0.4496 - val_accuracy: 0.8840\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2857 - accuracy: 0.9065 - val_loss: 0.5174 - val_accuracy: 0.8873\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2876 - accuracy: 0.9066 - val_loss: 0.5054 - val_accuracy: 0.8622\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2815 - accuracy: 0.9090 - val_loss: 0.5447 - val_accuracy: 0.8816\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2759 - accuracy: 0.9084 - val_loss: 0.5472 - val_accuracy: 0.8786\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2722 - accuracy: 0.9116 - val_loss: 0.5670 - val_accuracy: 0.8756\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2711 - accuracy: 0.9093 - val_loss: 0.5649 - val_accuracy: 0.8829\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2713 - accuracy: 0.9116 - val_loss: 0.5549 - val_accuracy: 0.8849\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2692 - accuracy: 0.9128 - val_loss: 0.5918 - val_accuracy: 0.8753\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2663 - accuracy: 0.9150 - val_loss: 0.5857 - val_accuracy: 0.8767\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2640 - accuracy: 0.9150 - val_loss: 0.6963 - val_accuracy: 0.8779\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2669 - accuracy: 0.9160 - val_loss: 0.6651 - val_accuracy: 0.8818\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2621 - accuracy: 0.9161 - val_loss: 0.7541 - val_accuracy: 0.8859\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2573 - accuracy: 0.9183 - val_loss: 0.7449 - val_accuracy: 0.8802\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2587 - accuracy: 0.9197 - val_loss: 0.6519 - val_accuracy: 0.8831\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2553 - accuracy: 0.9198 - val_loss: 0.7189 - val_accuracy: 0.8810\n",
      "313/313 - 0s - loss: 0.8136 - accuracy: 0.8713 - 244ms/epoch - 779us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.9197916388511658\n",
      "Validation accuracy : 0.8809999823570251\n",
      "Loss : 0.8136222958564758\n",
      "Accuracy : 0.8712999820709229\n",
      "\n",
      "Train Accuracy: 0.9197916388511658, Validation Accuracy: 0.8809999823570251\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with RMSprop, LR: 0.001, Batch Size: 64\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.5348 - accuracy: 0.8062 - val_loss: 0.4052 - val_accuracy: 0.8529\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3828 - accuracy: 0.8594 - val_loss: 0.3878 - val_accuracy: 0.8603\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3438 - accuracy: 0.8743 - val_loss: 0.3878 - val_accuracy: 0.8622\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3209 - accuracy: 0.8812 - val_loss: 0.3771 - val_accuracy: 0.8639\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3062 - accuracy: 0.8875 - val_loss: 0.3587 - val_accuracy: 0.8766\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2899 - accuracy: 0.8943 - val_loss: 0.3606 - val_accuracy: 0.8788\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2798 - accuracy: 0.8982 - val_loss: 0.3835 - val_accuracy: 0.8724\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2725 - accuracy: 0.9004 - val_loss: 0.3495 - val_accuracy: 0.8853\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2598 - accuracy: 0.9039 - val_loss: 0.3511 - val_accuracy: 0.8831\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.2571 - accuracy: 0.9053 - val_loss: 0.3782 - val_accuracy: 0.8823\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.2483 - accuracy: 0.9093 - val_loss: 0.3636 - val_accuracy: 0.8840\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.2452 - accuracy: 0.9105 - val_loss: 0.3802 - val_accuracy: 0.8882\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2401 - accuracy: 0.9120 - val_loss: 0.3912 - val_accuracy: 0.8894\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2342 - accuracy: 0.9156 - val_loss: 0.3960 - val_accuracy: 0.8886\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2296 - accuracy: 0.9170 - val_loss: 0.5028 - val_accuracy: 0.8577\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.2252 - accuracy: 0.9168 - val_loss: 0.4275 - val_accuracy: 0.8808\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.2235 - accuracy: 0.9197 - val_loss: 0.4258 - val_accuracy: 0.8840\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.2161 - accuracy: 0.9220 - val_loss: 0.4724 - val_accuracy: 0.8830\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2144 - accuracy: 0.9222 - val_loss: 0.4270 - val_accuracy: 0.8907\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2098 - accuracy: 0.9230 - val_loss: 0.4357 - val_accuracy: 0.8848\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2072 - accuracy: 0.9259 - val_loss: 0.4465 - val_accuracy: 0.8868\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2020 - accuracy: 0.9260 - val_loss: 0.5279 - val_accuracy: 0.8877\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1996 - accuracy: 0.9275 - val_loss: 0.5397 - val_accuracy: 0.8750\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1987 - accuracy: 0.9283 - val_loss: 0.5115 - val_accuracy: 0.8827\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1916 - accuracy: 0.9307 - val_loss: 0.5178 - val_accuracy: 0.8911\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1940 - accuracy: 0.9309 - val_loss: 0.5988 - val_accuracy: 0.8728\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1872 - accuracy: 0.9333 - val_loss: 0.5369 - val_accuracy: 0.8898\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1888 - accuracy: 0.9339 - val_loss: 0.5363 - val_accuracy: 0.8904\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1835 - accuracy: 0.9345 - val_loss: 0.5703 - val_accuracy: 0.8900\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1819 - accuracy: 0.9358 - val_loss: 0.5110 - val_accuracy: 0.8920\n",
      "313/313 - 0s - loss: 0.5870 - accuracy: 0.8876 - 180ms/epoch - 576us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.9358333349227905\n",
      "Validation accuracy : 0.8920000195503235\n",
      "Loss : 0.5870239734649658\n",
      "Accuracy : 0.8876000046730042\n",
      "\n",
      "Train Accuracy: 0.9358333349227905, Validation Accuracy: 0.8920000195503235\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with RMSprop, LR: 0.001, Batch Size: 128\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5938 - accuracy: 0.7899 - val_loss: 0.4128 - val_accuracy: 0.8499\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4022 - accuracy: 0.8535 - val_loss: 0.3924 - val_accuracy: 0.8521\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3533 - accuracy: 0.8696 - val_loss: 0.3858 - val_accuracy: 0.8611\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3250 - accuracy: 0.8792 - val_loss: 0.3426 - val_accuracy: 0.8794\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3039 - accuracy: 0.8869 - val_loss: 0.3550 - val_accuracy: 0.8768\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2864 - accuracy: 0.8936 - val_loss: 0.3202 - val_accuracy: 0.8870\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2725 - accuracy: 0.8992 - val_loss: 0.3250 - val_accuracy: 0.8841\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2585 - accuracy: 0.9034 - val_loss: 0.3306 - val_accuracy: 0.8852\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.9060 - val_loss: 0.3296 - val_accuracy: 0.8855\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2383 - accuracy: 0.9099 - val_loss: 0.3917 - val_accuracy: 0.8689\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2287 - accuracy: 0.9137 - val_loss: 0.3297 - val_accuracy: 0.8870\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9150 - val_loss: 0.3421 - val_accuracy: 0.8892\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2154 - accuracy: 0.9177 - val_loss: 0.3383 - val_accuracy: 0.8863\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2078 - accuracy: 0.9209 - val_loss: 0.3670 - val_accuracy: 0.8845\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1999 - accuracy: 0.9235 - val_loss: 0.3279 - val_accuracy: 0.8907\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1928 - accuracy: 0.9269 - val_loss: 0.3558 - val_accuracy: 0.8932\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1901 - accuracy: 0.9288 - val_loss: 0.3649 - val_accuracy: 0.8859\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1813 - accuracy: 0.9313 - val_loss: 0.3795 - val_accuracy: 0.8863\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1769 - accuracy: 0.9347 - val_loss: 0.4069 - val_accuracy: 0.8779\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1731 - accuracy: 0.9350 - val_loss: 0.4354 - val_accuracy: 0.8842\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1674 - accuracy: 0.9368 - val_loss: 0.3797 - val_accuracy: 0.8881\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1632 - accuracy: 0.9380 - val_loss: 0.4077 - val_accuracy: 0.8912\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1604 - accuracy: 0.9394 - val_loss: 0.4047 - val_accuracy: 0.8953\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1580 - accuracy: 0.9398 - val_loss: 0.4014 - val_accuracy: 0.8912\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1533 - accuracy: 0.9425 - val_loss: 0.4096 - val_accuracy: 0.8922\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.1495 - accuracy: 0.9436 - val_loss: 0.4527 - val_accuracy: 0.8872\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1454 - accuracy: 0.9450 - val_loss: 0.5196 - val_accuracy: 0.8829\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1436 - accuracy: 0.9460 - val_loss: 0.4486 - val_accuracy: 0.8842\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1421 - accuracy: 0.9473 - val_loss: 0.4741 - val_accuracy: 0.8886\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1374 - accuracy: 0.9480 - val_loss: 0.4518 - val_accuracy: 0.8912\n",
      "313/313 - 0s - loss: 0.5228 - accuracy: 0.8834 - 184ms/epoch - 589us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.9480208158493042\n",
      "Validation accuracy : 0.8911666870117188\n",
      "Loss : 0.5228369832038879\n",
      "Accuracy : 0.883400022983551\n",
      "\n",
      "Train Accuracy: 0.9480208158493042, Validation Accuracy: 0.8911666870117188\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with RMSprop, LR: 0.0005, Batch Size: 32\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5239 - accuracy: 0.8143 - val_loss: 0.3884 - val_accuracy: 0.8554\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3816 - accuracy: 0.8628 - val_loss: 0.3665 - val_accuracy: 0.8662\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3441 - accuracy: 0.8753 - val_loss: 0.3758 - val_accuracy: 0.8623\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3219 - accuracy: 0.8835 - val_loss: 0.3520 - val_accuracy: 0.8777\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3089 - accuracy: 0.8903 - val_loss: 0.3372 - val_accuracy: 0.8841\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2950 - accuracy: 0.8939 - val_loss: 0.3926 - val_accuracy: 0.8733\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2874 - accuracy: 0.8970 - val_loss: 0.3323 - val_accuracy: 0.8877\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2755 - accuracy: 0.9007 - val_loss: 0.3735 - val_accuracy: 0.8803\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.2687 - accuracy: 0.9036 - val_loss: 0.3652 - val_accuracy: 0.8838\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.2632 - accuracy: 0.9059 - val_loss: 0.3751 - val_accuracy: 0.8796\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2587 - accuracy: 0.9076 - val_loss: 0.3690 - val_accuracy: 0.8843\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2522 - accuracy: 0.9109 - val_loss: 0.3866 - val_accuracy: 0.8813\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2471 - accuracy: 0.9115 - val_loss: 0.4089 - val_accuracy: 0.8842\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2436 - accuracy: 0.9151 - val_loss: 0.4072 - val_accuracy: 0.8793\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2391 - accuracy: 0.9153 - val_loss: 0.3946 - val_accuracy: 0.8848\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2353 - accuracy: 0.9165 - val_loss: 0.4514 - val_accuracy: 0.8856\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2307 - accuracy: 0.9189 - val_loss: 0.4722 - val_accuracy: 0.8852\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2274 - accuracy: 0.9189 - val_loss: 0.5263 - val_accuracy: 0.8729\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2254 - accuracy: 0.9202 - val_loss: 0.4288 - val_accuracy: 0.8850\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2211 - accuracy: 0.9224 - val_loss: 0.4484 - val_accuracy: 0.8827\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2203 - accuracy: 0.9233 - val_loss: 0.4961 - val_accuracy: 0.8873\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2168 - accuracy: 0.9237 - val_loss: 0.5068 - val_accuracy: 0.8847\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2128 - accuracy: 0.9252 - val_loss: 0.5246 - val_accuracy: 0.8744\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2110 - accuracy: 0.9270 - val_loss: 0.5213 - val_accuracy: 0.8808\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2051 - accuracy: 0.9304 - val_loss: 0.5029 - val_accuracy: 0.8907\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2054 - accuracy: 0.9298 - val_loss: 0.5465 - val_accuracy: 0.8668\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2026 - accuracy: 0.9295 - val_loss: 0.5149 - val_accuracy: 0.8892\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1983 - accuracy: 0.9322 - val_loss: 0.5747 - val_accuracy: 0.8742\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1973 - accuracy: 0.9320 - val_loss: 0.5396 - val_accuracy: 0.8852\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1974 - accuracy: 0.9331 - val_loss: 0.5747 - val_accuracy: 0.8885\n",
      "313/313 - 0s - loss: 0.6643 - accuracy: 0.8831 - 246ms/epoch - 784us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.9331041574478149\n",
      "Validation accuracy : 0.8884999752044678\n",
      "Loss : 0.6642686724662781\n",
      "Accuracy : 0.8830999732017517\n",
      "\n",
      "Train Accuracy: 0.9331041574478149, Validation Accuracy: 0.8884999752044678\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with RMSprop, LR: 0.0005, Batch Size: 64\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.5520 - accuracy: 0.8058 - val_loss: 0.4615 - val_accuracy: 0.8263\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3886 - accuracy: 0.8615 - val_loss: 0.4425 - val_accuracy: 0.8369\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3443 - accuracy: 0.8739 - val_loss: 0.3463 - val_accuracy: 0.8769\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3181 - accuracy: 0.8827 - val_loss: 0.3334 - val_accuracy: 0.8798\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2993 - accuracy: 0.8899 - val_loss: 0.3175 - val_accuracy: 0.8838\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2835 - accuracy: 0.8944 - val_loss: 0.3301 - val_accuracy: 0.8847\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2706 - accuracy: 0.9000 - val_loss: 0.3288 - val_accuracy: 0.8833\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.2578 - accuracy: 0.9061 - val_loss: 0.3218 - val_accuracy: 0.8885\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2477 - accuracy: 0.9090 - val_loss: 0.3367 - val_accuracy: 0.8829\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.2381 - accuracy: 0.9119 - val_loss: 0.3706 - val_accuracy: 0.8766\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.2312 - accuracy: 0.9145 - val_loss: 0.3177 - val_accuracy: 0.8905\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2223 - accuracy: 0.9173 - val_loss: 0.3115 - val_accuracy: 0.8946\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2142 - accuracy: 0.9204 - val_loss: 0.3240 - val_accuracy: 0.8926\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2088 - accuracy: 0.9222 - val_loss: 0.3259 - val_accuracy: 0.8900\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2026 - accuracy: 0.9252 - val_loss: 0.3228 - val_accuracy: 0.8938\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1951 - accuracy: 0.9277 - val_loss: 0.3491 - val_accuracy: 0.8903\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1900 - accuracy: 0.9297 - val_loss: 0.3452 - val_accuracy: 0.8938\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1848 - accuracy: 0.9319 - val_loss: 0.3532 - val_accuracy: 0.8911\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1800 - accuracy: 0.9337 - val_loss: 0.3350 - val_accuracy: 0.8932\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1762 - accuracy: 0.9340 - val_loss: 0.3695 - val_accuracy: 0.8917\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1699 - accuracy: 0.9370 - val_loss: 0.3617 - val_accuracy: 0.8939\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1659 - accuracy: 0.9385 - val_loss: 0.4178 - val_accuracy: 0.8867\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1612 - accuracy: 0.9412 - val_loss: 0.3767 - val_accuracy: 0.8924\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1589 - accuracy: 0.9411 - val_loss: 0.3731 - val_accuracy: 0.8904\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1518 - accuracy: 0.9433 - val_loss: 0.3796 - val_accuracy: 0.8941\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.1496 - accuracy: 0.9456 - val_loss: 0.4350 - val_accuracy: 0.8911\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1464 - accuracy: 0.9462 - val_loss: 0.4008 - val_accuracy: 0.8920\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.1427 - accuracy: 0.9456 - val_loss: 0.4255 - val_accuracy: 0.8892\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1395 - accuracy: 0.9485 - val_loss: 0.5025 - val_accuracy: 0.8844\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1360 - accuracy: 0.9503 - val_loss: 0.4631 - val_accuracy: 0.8913\n",
      "313/313 - 0s - loss: 0.5023 - accuracy: 0.8845 - 179ms/epoch - 572us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.9502916932106018\n",
      "Validation accuracy : 0.8912500143051147\n",
      "Loss : 0.5022756457328796\n",
      "Accuracy : 0.8845000267028809\n",
      "\n",
      "Train Accuracy: 0.9502916932106018, Validation Accuracy: 0.8912500143051147\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with RMSprop, LR: 0.0005, Batch Size: 128\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5979 - accuracy: 0.7912 - val_loss: 0.5228 - val_accuracy: 0.8053\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4220 - accuracy: 0.8496 - val_loss: 0.3841 - val_accuracy: 0.8635\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3718 - accuracy: 0.8652 - val_loss: 0.4181 - val_accuracy: 0.8428\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3416 - accuracy: 0.8754 - val_loss: 0.3696 - val_accuracy: 0.8675\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3184 - accuracy: 0.8843 - val_loss: 0.3465 - val_accuracy: 0.8722\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3021 - accuracy: 0.8892 - val_loss: 0.3542 - val_accuracy: 0.8706\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2866 - accuracy: 0.8952 - val_loss: 0.3323 - val_accuracy: 0.8761\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2736 - accuracy: 0.8994 - val_loss: 0.3181 - val_accuracy: 0.8852\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2630 - accuracy: 0.9022 - val_loss: 0.3216 - val_accuracy: 0.8867\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2506 - accuracy: 0.9079 - val_loss: 0.3078 - val_accuracy: 0.8900\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2433 - accuracy: 0.9099 - val_loss: 0.3447 - val_accuracy: 0.8818\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2341 - accuracy: 0.9130 - val_loss: 0.3276 - val_accuracy: 0.8810\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2249 - accuracy: 0.9173 - val_loss: 0.3162 - val_accuracy: 0.8892\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2186 - accuracy: 0.9189 - val_loss: 0.3250 - val_accuracy: 0.8864\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2096 - accuracy: 0.9227 - val_loss: 0.3291 - val_accuracy: 0.8896\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2028 - accuracy: 0.9241 - val_loss: 0.3234 - val_accuracy: 0.8882\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1959 - accuracy: 0.9275 - val_loss: 0.3230 - val_accuracy: 0.8880\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1917 - accuracy: 0.9291 - val_loss: 0.3262 - val_accuracy: 0.8889\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1848 - accuracy: 0.9316 - val_loss: 0.3204 - val_accuracy: 0.8914\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1791 - accuracy: 0.9326 - val_loss: 0.3276 - val_accuracy: 0.8921\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1735 - accuracy: 0.9358 - val_loss: 0.3213 - val_accuracy: 0.8928\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1670 - accuracy: 0.9385 - val_loss: 0.3243 - val_accuracy: 0.8917\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1629 - accuracy: 0.9395 - val_loss: 0.3589 - val_accuracy: 0.8870\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1597 - accuracy: 0.9406 - val_loss: 0.3724 - val_accuracy: 0.8809\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1542 - accuracy: 0.9427 - val_loss: 0.3410 - val_accuracy: 0.8961\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1488 - accuracy: 0.9442 - val_loss: 0.3383 - val_accuracy: 0.8923\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1440 - accuracy: 0.9464 - val_loss: 0.3635 - val_accuracy: 0.8913\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1405 - accuracy: 0.9484 - val_loss: 0.3672 - val_accuracy: 0.8943\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1373 - accuracy: 0.9490 - val_loss: 0.3551 - val_accuracy: 0.8929\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1319 - accuracy: 0.9510 - val_loss: 0.3632 - val_accuracy: 0.8917\n",
      "313/313 - 0s - loss: 0.3939 - accuracy: 0.8889 - 309ms/epoch - 987us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.9510416388511658\n",
      "Validation accuracy : 0.8917499780654907\n",
      "Loss : 0.3939184844493866\n",
      "Accuracy : 0.8888999819755554\n",
      "\n",
      "Train Accuracy: 0.9510416388511658, Validation Accuracy: 0.8917499780654907\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with RMSprop, LR: 0.0001, Batch Size: 32\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6531 - accuracy: 0.7894 - val_loss: 0.4814 - val_accuracy: 0.8338\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4457 - accuracy: 0.8462 - val_loss: 0.4235 - val_accuracy: 0.8520\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4049 - accuracy: 0.8599 - val_loss: 0.3949 - val_accuracy: 0.8622\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3776 - accuracy: 0.8676 - val_loss: 0.3831 - val_accuracy: 0.8662\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3593 - accuracy: 0.8732 - val_loss: 0.3815 - val_accuracy: 0.8654\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3446 - accuracy: 0.8787 - val_loss: 0.3627 - val_accuracy: 0.8743\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3318 - accuracy: 0.8827 - val_loss: 0.3562 - val_accuracy: 0.8793\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3201 - accuracy: 0.8860 - val_loss: 0.3510 - val_accuracy: 0.8774\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3099 - accuracy: 0.8901 - val_loss: 0.3451 - val_accuracy: 0.8794\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3013 - accuracy: 0.8925 - val_loss: 0.3423 - val_accuracy: 0.8817\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2938 - accuracy: 0.8952 - val_loss: 0.3358 - val_accuracy: 0.8803\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2864 - accuracy: 0.8972 - val_loss: 0.3448 - val_accuracy: 0.8744\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2800 - accuracy: 0.8992 - val_loss: 0.3377 - val_accuracy: 0.8808\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2733 - accuracy: 0.9023 - val_loss: 0.3342 - val_accuracy: 0.8829\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2665 - accuracy: 0.9041 - val_loss: 0.3261 - val_accuracy: 0.8849\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2620 - accuracy: 0.9062 - val_loss: 0.3376 - val_accuracy: 0.8795\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2571 - accuracy: 0.9076 - val_loss: 0.3290 - val_accuracy: 0.8853\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2523 - accuracy: 0.9112 - val_loss: 0.3273 - val_accuracy: 0.8878\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2473 - accuracy: 0.9130 - val_loss: 0.3246 - val_accuracy: 0.8873\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2413 - accuracy: 0.9137 - val_loss: 0.3184 - val_accuracy: 0.8882\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2380 - accuracy: 0.9151 - val_loss: 0.3442 - val_accuracy: 0.8811\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2333 - accuracy: 0.9167 - val_loss: 0.3144 - val_accuracy: 0.8913\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2283 - accuracy: 0.9189 - val_loss: 0.3157 - val_accuracy: 0.8902\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2251 - accuracy: 0.9194 - val_loss: 0.3172 - val_accuracy: 0.8903\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2210 - accuracy: 0.9205 - val_loss: 0.3249 - val_accuracy: 0.8919\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2175 - accuracy: 0.9217 - val_loss: 0.3256 - val_accuracy: 0.8913\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2132 - accuracy: 0.9233 - val_loss: 0.3135 - val_accuracy: 0.8916\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2094 - accuracy: 0.9256 - val_loss: 0.3281 - val_accuracy: 0.8891\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2057 - accuracy: 0.9275 - val_loss: 0.3230 - val_accuracy: 0.8903\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2024 - accuracy: 0.9290 - val_loss: 0.3259 - val_accuracy: 0.8908\n",
      "313/313 - 0s - loss: 0.3508 - accuracy: 0.8852 - 244ms/epoch - 779us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.9289583563804626\n",
      "Validation accuracy : 0.89083331823349\n",
      "Loss : 0.3508061468601227\n",
      "Accuracy : 0.885200023651123\n",
      "\n",
      "Train Accuracy: 0.9289583563804626, Validation Accuracy: 0.89083331823349\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with RMSprop, LR: 0.0001, Batch Size: 64\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.7473 - accuracy: 0.7650 - val_loss: 0.5234 - val_accuracy: 0.8224\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4764 - accuracy: 0.8374 - val_loss: 0.4453 - val_accuracy: 0.8435\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4279 - accuracy: 0.8515 - val_loss: 0.4424 - val_accuracy: 0.8454\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4001 - accuracy: 0.8610 - val_loss: 0.4074 - val_accuracy: 0.8554\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3808 - accuracy: 0.8648 - val_loss: 0.3882 - val_accuracy: 0.8633\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3650 - accuracy: 0.8713 - val_loss: 0.3750 - val_accuracy: 0.8671\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3516 - accuracy: 0.8758 - val_loss: 0.3685 - val_accuracy: 0.8716\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3408 - accuracy: 0.8802 - val_loss: 0.3747 - val_accuracy: 0.8664\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3306 - accuracy: 0.8841 - val_loss: 0.3641 - val_accuracy: 0.8708\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3215 - accuracy: 0.8864 - val_loss: 0.3529 - val_accuracy: 0.8740\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3139 - accuracy: 0.8876 - val_loss: 0.3446 - val_accuracy: 0.8771\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3062 - accuracy: 0.8906 - val_loss: 0.3412 - val_accuracy: 0.8787\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2993 - accuracy: 0.8930 - val_loss: 0.3334 - val_accuracy: 0.8836\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2924 - accuracy: 0.8956 - val_loss: 0.3449 - val_accuracy: 0.8779\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2874 - accuracy: 0.8970 - val_loss: 0.3483 - val_accuracy: 0.8778\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.2814 - accuracy: 0.8995 - val_loss: 0.3345 - val_accuracy: 0.8808\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.2761 - accuracy: 0.9004 - val_loss: 0.3298 - val_accuracy: 0.8838\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2713 - accuracy: 0.9015 - val_loss: 0.3245 - val_accuracy: 0.8851\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2666 - accuracy: 0.9031 - val_loss: 0.3227 - val_accuracy: 0.8860\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.2610 - accuracy: 0.9059 - val_loss: 0.3300 - val_accuracy: 0.8838\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2569 - accuracy: 0.9072 - val_loss: 0.3194 - val_accuracy: 0.8844\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2525 - accuracy: 0.9091 - val_loss: 0.3334 - val_accuracy: 0.8824\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2489 - accuracy: 0.9097 - val_loss: 0.3350 - val_accuracy: 0.8788\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2451 - accuracy: 0.9117 - val_loss: 0.3224 - val_accuracy: 0.8860\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2412 - accuracy: 0.9122 - val_loss: 0.3182 - val_accuracy: 0.8868\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.2378 - accuracy: 0.9148 - val_loss: 0.3174 - val_accuracy: 0.8876\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.2339 - accuracy: 0.9164 - val_loss: 0.3154 - val_accuracy: 0.8881\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.2305 - accuracy: 0.9164 - val_loss: 0.3224 - val_accuracy: 0.8873\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.2261 - accuracy: 0.9175 - val_loss: 0.3138 - val_accuracy: 0.8884\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.2227 - accuracy: 0.9207 - val_loss: 0.3253 - val_accuracy: 0.8877\n",
      "313/313 - 0s - loss: 0.3475 - accuracy: 0.8822 - 178ms/epoch - 570us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.9206874966621399\n",
      "Validation accuracy : 0.887666642665863\n",
      "Loss : 0.34754419326782227\n",
      "Accuracy : 0.8822000026702881\n",
      "\n",
      "Train Accuracy: 0.9206874966621399, Validation Accuracy: 0.887666642665863\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with RMSprop, LR: 0.0001, Batch Size: 128\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.8446 - accuracy: 0.7464 - val_loss: 0.5609 - val_accuracy: 0.8133\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.8296 - val_loss: 0.4829 - val_accuracy: 0.8348\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4550 - accuracy: 0.8447 - val_loss: 0.4376 - val_accuracy: 0.8523\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4251 - accuracy: 0.8548 - val_loss: 0.4244 - val_accuracy: 0.8505\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4040 - accuracy: 0.8596 - val_loss: 0.4073 - val_accuracy: 0.8602\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3884 - accuracy: 0.8663 - val_loss: 0.3900 - val_accuracy: 0.8633\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3747 - accuracy: 0.8703 - val_loss: 0.3857 - val_accuracy: 0.8647\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3641 - accuracy: 0.8728 - val_loss: 0.3786 - val_accuracy: 0.8675\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3548 - accuracy: 0.8764 - val_loss: 0.3677 - val_accuracy: 0.8717\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3459 - accuracy: 0.8794 - val_loss: 0.3648 - val_accuracy: 0.8717\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3389 - accuracy: 0.8815 - val_loss: 0.3565 - val_accuracy: 0.8756\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3315 - accuracy: 0.8836 - val_loss: 0.3583 - val_accuracy: 0.8741\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3242 - accuracy: 0.8861 - val_loss: 0.3512 - val_accuracy: 0.8760\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3184 - accuracy: 0.8867 - val_loss: 0.3513 - val_accuracy: 0.8764\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3128 - accuracy: 0.8894 - val_loss: 0.3504 - val_accuracy: 0.8770\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3077 - accuracy: 0.8916 - val_loss: 0.3462 - val_accuracy: 0.8771\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3021 - accuracy: 0.8926 - val_loss: 0.3573 - val_accuracy: 0.8716\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2973 - accuracy: 0.8940 - val_loss: 0.3364 - val_accuracy: 0.8822\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2939 - accuracy: 0.8960 - val_loss: 0.3348 - val_accuracy: 0.8822\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2893 - accuracy: 0.8967 - val_loss: 0.3330 - val_accuracy: 0.8812\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2848 - accuracy: 0.8972 - val_loss: 0.3336 - val_accuracy: 0.8808\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2808 - accuracy: 0.8992 - val_loss: 0.3227 - val_accuracy: 0.8840\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2764 - accuracy: 0.9012 - val_loss: 0.3289 - val_accuracy: 0.8812\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2727 - accuracy: 0.9029 - val_loss: 0.3300 - val_accuracy: 0.8827\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2690 - accuracy: 0.9049 - val_loss: 0.3242 - val_accuracy: 0.8848\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2656 - accuracy: 0.9053 - val_loss: 0.3301 - val_accuracy: 0.8811\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2621 - accuracy: 0.9069 - val_loss: 0.3213 - val_accuracy: 0.8844\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2587 - accuracy: 0.9074 - val_loss: 0.3231 - val_accuracy: 0.8834\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2551 - accuracy: 0.9092 - val_loss: 0.3166 - val_accuracy: 0.8865\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2530 - accuracy: 0.9097 - val_loss: 0.3145 - val_accuracy: 0.8891\n",
      "313/313 - 0s - loss: 0.3404 - accuracy: 0.8788 - 184ms/epoch - 587us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.9097499847412109\n",
      "Validation accuracy : 0.8890833258628845\n",
      "Loss : 0.34039992094039917\n",
      "Accuracy : 0.8787999749183655\n",
      "\n",
      "Train Accuracy: 0.9097499847412109, Validation Accuracy: 0.8890833258628845\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adam, LR: 0.001, Batch Size: 32\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5059 - accuracy: 0.8191 - val_loss: 0.4049 - val_accuracy: 0.8572\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3734 - accuracy: 0.8633 - val_loss: 0.3554 - val_accuracy: 0.8713\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3348 - accuracy: 0.8767 - val_loss: 0.3542 - val_accuracy: 0.8725\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3122 - accuracy: 0.8838 - val_loss: 0.3227 - val_accuracy: 0.8827\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2929 - accuracy: 0.8928 - val_loss: 0.3459 - val_accuracy: 0.8777\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2794 - accuracy: 0.8953 - val_loss: 0.3285 - val_accuracy: 0.8846\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2656 - accuracy: 0.9003 - val_loss: 0.3224 - val_accuracy: 0.8847\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2500 - accuracy: 0.9061 - val_loss: 0.3589 - val_accuracy: 0.8712\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2405 - accuracy: 0.9096 - val_loss: 0.3269 - val_accuracy: 0.8873\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2315 - accuracy: 0.9118 - val_loss: 0.3181 - val_accuracy: 0.8905\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2215 - accuracy: 0.9157 - val_loss: 0.3566 - val_accuracy: 0.8804\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2148 - accuracy: 0.9177 - val_loss: 0.3490 - val_accuracy: 0.8826\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2053 - accuracy: 0.9212 - val_loss: 0.3466 - val_accuracy: 0.8874\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1980 - accuracy: 0.9255 - val_loss: 0.3290 - val_accuracy: 0.8945\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1906 - accuracy: 0.9272 - val_loss: 0.3428 - val_accuracy: 0.8930\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1850 - accuracy: 0.9288 - val_loss: 0.3508 - val_accuracy: 0.8879\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1805 - accuracy: 0.9306 - val_loss: 0.3471 - val_accuracy: 0.8848\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1746 - accuracy: 0.9340 - val_loss: 0.3456 - val_accuracy: 0.8924\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1699 - accuracy: 0.9363 - val_loss: 0.3598 - val_accuracy: 0.8896\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1643 - accuracy: 0.9375 - val_loss: 0.3615 - val_accuracy: 0.8923\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1610 - accuracy: 0.9375 - val_loss: 0.3809 - val_accuracy: 0.8900\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1572 - accuracy: 0.9405 - val_loss: 0.4182 - val_accuracy: 0.8829\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1534 - accuracy: 0.9412 - val_loss: 0.3884 - val_accuracy: 0.8928\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1470 - accuracy: 0.9435 - val_loss: 0.3730 - val_accuracy: 0.8892\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1420 - accuracy: 0.9456 - val_loss: 0.4016 - val_accuracy: 0.8954\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1389 - accuracy: 0.9473 - val_loss: 0.3922 - val_accuracy: 0.8963\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1349 - accuracy: 0.9484 - val_loss: 0.4279 - val_accuracy: 0.8918\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1328 - accuracy: 0.9488 - val_loss: 0.3973 - val_accuracy: 0.8932\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1285 - accuracy: 0.9510 - val_loss: 0.4209 - val_accuracy: 0.8896\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1280 - accuracy: 0.9507 - val_loss: 0.4347 - val_accuracy: 0.8854\n",
      "313/313 - 0s - loss: 0.4707 - accuracy: 0.8811 - 237ms/epoch - 756us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.9506666660308838\n",
      "Validation accuracy : 0.8854166865348816\n",
      "Loss : 0.4706507921218872\n",
      "Accuracy : 0.8810999989509583\n",
      "\n",
      "Train Accuracy: 0.9506666660308838, Validation Accuracy: 0.8854166865348816\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adam, LR: 0.001, Batch Size: 64\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.5081 - accuracy: 0.8209 - val_loss: 0.4159 - val_accuracy: 0.8522\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.3699 - accuracy: 0.8664 - val_loss: 0.3526 - val_accuracy: 0.8699\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.3322 - accuracy: 0.8777 - val_loss: 0.3798 - val_accuracy: 0.8650\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.3077 - accuracy: 0.8860 - val_loss: 0.3331 - val_accuracy: 0.8800\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.2877 - accuracy: 0.8934 - val_loss: 0.3332 - val_accuracy: 0.8802\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.2758 - accuracy: 0.8972 - val_loss: 0.3322 - val_accuracy: 0.8838\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.2601 - accuracy: 0.9021 - val_loss: 0.3133 - val_accuracy: 0.8863\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.2507 - accuracy: 0.9063 - val_loss: 0.3394 - val_accuracy: 0.8783\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.2386 - accuracy: 0.9096 - val_loss: 0.3129 - val_accuracy: 0.8898\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.2320 - accuracy: 0.9118 - val_loss: 0.3158 - val_accuracy: 0.8904\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.2204 - accuracy: 0.9158 - val_loss: 0.3342 - val_accuracy: 0.8889\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.2134 - accuracy: 0.9195 - val_loss: 0.3180 - val_accuracy: 0.8893\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.2042 - accuracy: 0.9234 - val_loss: 0.3274 - val_accuracy: 0.8888\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.1965 - accuracy: 0.9261 - val_loss: 0.3103 - val_accuracy: 0.8949\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.1911 - accuracy: 0.9273 - val_loss: 0.3461 - val_accuracy: 0.8898\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.1820 - accuracy: 0.9306 - val_loss: 0.3317 - val_accuracy: 0.8913\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1783 - accuracy: 0.9329 - val_loss: 0.3414 - val_accuracy: 0.8903\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1711 - accuracy: 0.9359 - val_loss: 0.3584 - val_accuracy: 0.8899\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1663 - accuracy: 0.9365 - val_loss: 0.3580 - val_accuracy: 0.8897\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.1647 - accuracy: 0.9373 - val_loss: 0.3365 - val_accuracy: 0.8928\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.1566 - accuracy: 0.9398 - val_loss: 0.3778 - val_accuracy: 0.8877\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1517 - accuracy: 0.9426 - val_loss: 0.3759 - val_accuracy: 0.8867\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1471 - accuracy: 0.9432 - val_loss: 0.3626 - val_accuracy: 0.8927\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.1437 - accuracy: 0.9447 - val_loss: 0.3939 - val_accuracy: 0.8916\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1401 - accuracy: 0.9473 - val_loss: 0.4021 - val_accuracy: 0.8868\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.1339 - accuracy: 0.9496 - val_loss: 0.3767 - val_accuracy: 0.8913\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1321 - accuracy: 0.9496 - val_loss: 0.3749 - val_accuracy: 0.8947\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.1279 - accuracy: 0.9519 - val_loss: 0.3885 - val_accuracy: 0.8907\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.1210 - accuracy: 0.9544 - val_loss: 0.3871 - val_accuracy: 0.8919\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.1202 - accuracy: 0.9539 - val_loss: 0.4068 - val_accuracy: 0.8947\n",
      "313/313 - 0s - loss: 0.4414 - accuracy: 0.8877 - 178ms/epoch - 569us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.9539166688919067\n",
      "Validation accuracy : 0.8947499990463257\n",
      "Loss : 0.4413725733757019\n",
      "Accuracy : 0.8877000212669373\n",
      "\n",
      "Train Accuracy: 0.9539166688919067, Validation Accuracy: 0.8947499990463257\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adam, LR: 0.001, Batch Size: 128\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5503 - accuracy: 0.8097 - val_loss: 0.4372 - val_accuracy: 0.8475\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3887 - accuracy: 0.8619 - val_loss: 0.3758 - val_accuracy: 0.8675\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3501 - accuracy: 0.8733 - val_loss: 0.3759 - val_accuracy: 0.8667\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3224 - accuracy: 0.8817 - val_loss: 0.3460 - val_accuracy: 0.8748\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3013 - accuracy: 0.8901 - val_loss: 0.3427 - val_accuracy: 0.8742\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2865 - accuracy: 0.8949 - val_loss: 0.3311 - val_accuracy: 0.8822\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2711 - accuracy: 0.8989 - val_loss: 0.3298 - val_accuracy: 0.8799\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2581 - accuracy: 0.9042 - val_loss: 0.3186 - val_accuracy: 0.8832\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2488 - accuracy: 0.9058 - val_loss: 0.3124 - val_accuracy: 0.8866\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2427 - accuracy: 0.9103 - val_loss: 0.3225 - val_accuracy: 0.8874\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2293 - accuracy: 0.9154 - val_loss: 0.3263 - val_accuracy: 0.8873\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2216 - accuracy: 0.9167 - val_loss: 0.3201 - val_accuracy: 0.8889\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2133 - accuracy: 0.9198 - val_loss: 0.3088 - val_accuracy: 0.8916\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2052 - accuracy: 0.9223 - val_loss: 0.3127 - val_accuracy: 0.8918\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1962 - accuracy: 0.9264 - val_loss: 0.3226 - val_accuracy: 0.8922\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1931 - accuracy: 0.9280 - val_loss: 0.3547 - val_accuracy: 0.8821\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1873 - accuracy: 0.9295 - val_loss: 0.3148 - val_accuracy: 0.8973\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1818 - accuracy: 0.9324 - val_loss: 0.3321 - val_accuracy: 0.8912\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1768 - accuracy: 0.9346 - val_loss: 0.3271 - val_accuracy: 0.8934\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1648 - accuracy: 0.9381 - val_loss: 0.3339 - val_accuracy: 0.8904\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1608 - accuracy: 0.9405 - val_loss: 0.3391 - val_accuracy: 0.8935\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1571 - accuracy: 0.9408 - val_loss: 0.3349 - val_accuracy: 0.8938\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1563 - accuracy: 0.9398 - val_loss: 0.3250 - val_accuracy: 0.8967\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1497 - accuracy: 0.9432 - val_loss: 0.3619 - val_accuracy: 0.8934\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1415 - accuracy: 0.9465 - val_loss: 0.3595 - val_accuracy: 0.8896\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1391 - accuracy: 0.9472 - val_loss: 0.3514 - val_accuracy: 0.8965\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1324 - accuracy: 0.9496 - val_loss: 0.3695 - val_accuracy: 0.8934\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1293 - accuracy: 0.9511 - val_loss: 0.3739 - val_accuracy: 0.8913\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1277 - accuracy: 0.9516 - val_loss: 0.3851 - val_accuracy: 0.8927\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1266 - accuracy: 0.9527 - val_loss: 0.4058 - val_accuracy: 0.8896\n",
      "313/313 - 0s - loss: 0.4304 - accuracy: 0.8854 - 185ms/epoch - 590us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.9526875019073486\n",
      "Validation accuracy : 0.8895833492279053\n",
      "Loss : 0.4304226040840149\n",
      "Accuracy : 0.8853999972343445\n",
      "\n",
      "Train Accuracy: 0.9526875019073486, Validation Accuracy: 0.8895833492279053\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adam, LR: 0.0005, Batch Size: 32\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5151 - accuracy: 0.8203 - val_loss: 0.4443 - val_accuracy: 0.8324\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3776 - accuracy: 0.8636 - val_loss: 0.3821 - val_accuracy: 0.8636\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3364 - accuracy: 0.8777 - val_loss: 0.3761 - val_accuracy: 0.8626\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3131 - accuracy: 0.8860 - val_loss: 0.3513 - val_accuracy: 0.8725\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2891 - accuracy: 0.8943 - val_loss: 0.3182 - val_accuracy: 0.8831\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2730 - accuracy: 0.8985 - val_loss: 0.3249 - val_accuracy: 0.8836\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2631 - accuracy: 0.9010 - val_loss: 0.3310 - val_accuracy: 0.8797\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2497 - accuracy: 0.9066 - val_loss: 0.3200 - val_accuracy: 0.8857\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2400 - accuracy: 0.9086 - val_loss: 0.3056 - val_accuracy: 0.8902\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2272 - accuracy: 0.9156 - val_loss: 0.3373 - val_accuracy: 0.8844\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2189 - accuracy: 0.9178 - val_loss: 0.3014 - val_accuracy: 0.8938\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2091 - accuracy: 0.9219 - val_loss: 0.3145 - val_accuracy: 0.8906\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2032 - accuracy: 0.9229 - val_loss: 0.2987 - val_accuracy: 0.8961\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1931 - accuracy: 0.9276 - val_loss: 0.3218 - val_accuracy: 0.8901\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1868 - accuracy: 0.9306 - val_loss: 0.3130 - val_accuracy: 0.8957\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1790 - accuracy: 0.9320 - val_loss: 0.3263 - val_accuracy: 0.8922\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1728 - accuracy: 0.9353 - val_loss: 0.3134 - val_accuracy: 0.8949\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1681 - accuracy: 0.9366 - val_loss: 0.3172 - val_accuracy: 0.8968\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1619 - accuracy: 0.9395 - val_loss: 0.3372 - val_accuracy: 0.8914\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1546 - accuracy: 0.9421 - val_loss: 0.3458 - val_accuracy: 0.8917\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1494 - accuracy: 0.9437 - val_loss: 0.3397 - val_accuracy: 0.8938\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1451 - accuracy: 0.9452 - val_loss: 0.3438 - val_accuracy: 0.8951\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1394 - accuracy: 0.9477 - val_loss: 0.3661 - val_accuracy: 0.8934\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1363 - accuracy: 0.9490 - val_loss: 0.3570 - val_accuracy: 0.8918\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1315 - accuracy: 0.9498 - val_loss: 0.3516 - val_accuracy: 0.8993\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1290 - accuracy: 0.9513 - val_loss: 0.3638 - val_accuracy: 0.8952\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1210 - accuracy: 0.9538 - val_loss: 0.3672 - val_accuracy: 0.8942\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1177 - accuracy: 0.9554 - val_loss: 0.3925 - val_accuracy: 0.8932\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1160 - accuracy: 0.9561 - val_loss: 0.3885 - val_accuracy: 0.8920\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.9582 - val_loss: 0.3924 - val_accuracy: 0.8955\n",
      "313/313 - 0s - loss: 0.4430 - accuracy: 0.8864 - 238ms/epoch - 762us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.9582499861717224\n",
      "Validation accuracy : 0.8955000042915344\n",
      "Loss : 0.4430149793624878\n",
      "Accuracy : 0.8863999843597412\n",
      "\n",
      "Train Accuracy: 0.9582499861717224, Validation Accuracy: 0.8955000042915344\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adam, LR: 0.0005, Batch Size: 64\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5473 - accuracy: 0.8102 - val_loss: 0.4413 - val_accuracy: 0.8455\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.3899 - accuracy: 0.8606 - val_loss: 0.3833 - val_accuracy: 0.8582\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3486 - accuracy: 0.8732 - val_loss: 0.3447 - val_accuracy: 0.8780\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3207 - accuracy: 0.8834 - val_loss: 0.3298 - val_accuracy: 0.8799\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3004 - accuracy: 0.8893 - val_loss: 0.3227 - val_accuracy: 0.8843\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2836 - accuracy: 0.8947 - val_loss: 0.3296 - val_accuracy: 0.8823\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2691 - accuracy: 0.9004 - val_loss: 0.3327 - val_accuracy: 0.8809\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.2595 - accuracy: 0.9040 - val_loss: 0.3185 - val_accuracy: 0.8844\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2457 - accuracy: 0.9084 - val_loss: 0.3082 - val_accuracy: 0.8926\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2365 - accuracy: 0.9124 - val_loss: 0.3137 - val_accuracy: 0.8856\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2255 - accuracy: 0.9153 - val_loss: 0.3221 - val_accuracy: 0.8844\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2180 - accuracy: 0.9185 - val_loss: 0.3107 - val_accuracy: 0.8893\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2103 - accuracy: 0.9217 - val_loss: 0.3217 - val_accuracy: 0.8895\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.2022 - accuracy: 0.9247 - val_loss: 0.3233 - val_accuracy: 0.8907\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.1965 - accuracy: 0.9262 - val_loss: 0.3408 - val_accuracy: 0.8876\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1888 - accuracy: 0.9293 - val_loss: 0.3285 - val_accuracy: 0.8900\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.1810 - accuracy: 0.9318 - val_loss: 0.3287 - val_accuracy: 0.8906\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1776 - accuracy: 0.9341 - val_loss: 0.3169 - val_accuracy: 0.8941\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1695 - accuracy: 0.9368 - val_loss: 0.3256 - val_accuracy: 0.8932\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1644 - accuracy: 0.9388 - val_loss: 0.3325 - val_accuracy: 0.8921\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1563 - accuracy: 0.9422 - val_loss: 0.3442 - val_accuracy: 0.8904\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1529 - accuracy: 0.9435 - val_loss: 0.3342 - val_accuracy: 0.8934\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1471 - accuracy: 0.9448 - val_loss: 0.3527 - val_accuracy: 0.8896\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1416 - accuracy: 0.9463 - val_loss: 0.3500 - val_accuracy: 0.8919\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1373 - accuracy: 0.9500 - val_loss: 0.3648 - val_accuracy: 0.8869\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1311 - accuracy: 0.9503 - val_loss: 0.3614 - val_accuracy: 0.8919\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1290 - accuracy: 0.9510 - val_loss: 0.3640 - val_accuracy: 0.8935\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1231 - accuracy: 0.9543 - val_loss: 0.3957 - val_accuracy: 0.8934\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1215 - accuracy: 0.9555 - val_loss: 0.3863 - val_accuracy: 0.8911\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.1163 - accuracy: 0.9566 - val_loss: 0.3735 - val_accuracy: 0.8919\n",
      "313/313 - 0s - loss: 0.3972 - accuracy: 0.8894 - 179ms/epoch - 573us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.9565625190734863\n",
      "Validation accuracy : 0.8919166922569275\n",
      "Loss : 0.39716464281082153\n",
      "Accuracy : 0.8894000053405762\n",
      "\n",
      "Train Accuracy: 0.9565625190734863, Validation Accuracy: 0.8919166922569275\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adam, LR: 0.0005, Batch Size: 128\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5815 - accuracy: 0.8040 - val_loss: 0.4754 - val_accuracy: 0.8322\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4072 - accuracy: 0.8569 - val_loss: 0.4110 - val_accuracy: 0.8583\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3616 - accuracy: 0.8721 - val_loss: 0.3620 - val_accuracy: 0.8723\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3325 - accuracy: 0.8804 - val_loss: 0.3503 - val_accuracy: 0.8755\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3160 - accuracy: 0.8863 - val_loss: 0.3669 - val_accuracy: 0.8702\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2971 - accuracy: 0.8917 - val_loss: 0.3400 - val_accuracy: 0.8777\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2815 - accuracy: 0.8976 - val_loss: 0.3284 - val_accuracy: 0.8832\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2737 - accuracy: 0.9009 - val_loss: 0.3277 - val_accuracy: 0.8852\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2602 - accuracy: 0.9047 - val_loss: 0.3103 - val_accuracy: 0.8881\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2531 - accuracy: 0.9067 - val_loss: 0.3141 - val_accuracy: 0.8871\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2413 - accuracy: 0.9113 - val_loss: 0.3153 - val_accuracy: 0.8872\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2346 - accuracy: 0.9144 - val_loss: 0.3087 - val_accuracy: 0.8880\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2265 - accuracy: 0.9175 - val_loss: 0.3143 - val_accuracy: 0.8905\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2204 - accuracy: 0.9185 - val_loss: 0.3328 - val_accuracy: 0.8799\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2135 - accuracy: 0.9211 - val_loss: 0.3040 - val_accuracy: 0.8935\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2055 - accuracy: 0.9242 - val_loss: 0.3123 - val_accuracy: 0.8896\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1989 - accuracy: 0.9265 - val_loss: 0.3231 - val_accuracy: 0.8901\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1920 - accuracy: 0.9285 - val_loss: 0.3022 - val_accuracy: 0.8934\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1888 - accuracy: 0.9308 - val_loss: 0.3137 - val_accuracy: 0.8928\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1799 - accuracy: 0.9337 - val_loss: 0.3152 - val_accuracy: 0.8938\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1736 - accuracy: 0.9364 - val_loss: 0.3126 - val_accuracy: 0.8958\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1682 - accuracy: 0.9378 - val_loss: 0.3167 - val_accuracy: 0.8916\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1616 - accuracy: 0.9412 - val_loss: 0.3101 - val_accuracy: 0.8947\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1573 - accuracy: 0.9428 - val_loss: 0.3365 - val_accuracy: 0.8875\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1552 - accuracy: 0.9425 - val_loss: 0.3146 - val_accuracy: 0.8960\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1500 - accuracy: 0.9442 - val_loss: 0.3263 - val_accuracy: 0.8950\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1481 - accuracy: 0.9454 - val_loss: 0.3160 - val_accuracy: 0.8963\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1391 - accuracy: 0.9490 - val_loss: 0.3228 - val_accuracy: 0.9019\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1345 - accuracy: 0.9501 - val_loss: 0.3610 - val_accuracy: 0.8876\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1327 - accuracy: 0.9517 - val_loss: 0.3419 - val_accuracy: 0.8952\n",
      "313/313 - 0s - loss: 0.3783 - accuracy: 0.8856 - 183ms/epoch - 584us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.9517083168029785\n",
      "Validation accuracy : 0.8951666951179504\n",
      "Loss : 0.3782574236392975\n",
      "Accuracy : 0.8855999708175659\n",
      "\n",
      "Train Accuracy: 0.9517083168029785, Validation Accuracy: 0.8951666951179504\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adam, LR: 0.0001, Batch Size: 32\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6493 - accuracy: 0.7865 - val_loss: 0.4760 - val_accuracy: 0.8393\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4373 - accuracy: 0.8495 - val_loss: 0.4277 - val_accuracy: 0.8485\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3953 - accuracy: 0.8620 - val_loss: 0.3951 - val_accuracy: 0.8611\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3691 - accuracy: 0.8706 - val_loss: 0.3848 - val_accuracy: 0.8626\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3499 - accuracy: 0.8756 - val_loss: 0.3660 - val_accuracy: 0.8702\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3339 - accuracy: 0.8809 - val_loss: 0.3609 - val_accuracy: 0.8678\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3209 - accuracy: 0.8850 - val_loss: 0.3511 - val_accuracy: 0.8722\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3107 - accuracy: 0.8894 - val_loss: 0.3436 - val_accuracy: 0.8751\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3001 - accuracy: 0.8930 - val_loss: 0.3348 - val_accuracy: 0.8783\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2919 - accuracy: 0.8952 - val_loss: 0.3333 - val_accuracy: 0.8808\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2827 - accuracy: 0.8981 - val_loss: 0.3237 - val_accuracy: 0.8854\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2738 - accuracy: 0.9013 - val_loss: 0.3222 - val_accuracy: 0.8862\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2663 - accuracy: 0.9034 - val_loss: 0.3190 - val_accuracy: 0.8863\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2593 - accuracy: 0.9061 - val_loss: 0.3204 - val_accuracy: 0.8858\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2527 - accuracy: 0.9082 - val_loss: 0.3216 - val_accuracy: 0.8834\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2459 - accuracy: 0.9106 - val_loss: 0.3169 - val_accuracy: 0.8866\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2412 - accuracy: 0.9130 - val_loss: 0.3171 - val_accuracy: 0.8865\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2340 - accuracy: 0.9160 - val_loss: 0.3246 - val_accuracy: 0.8821\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2296 - accuracy: 0.9162 - val_loss: 0.3154 - val_accuracy: 0.8883\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2235 - accuracy: 0.9191 - val_loss: 0.3113 - val_accuracy: 0.8903\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2189 - accuracy: 0.9215 - val_loss: 0.3052 - val_accuracy: 0.8910\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2133 - accuracy: 0.9231 - val_loss: 0.3129 - val_accuracy: 0.8900\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2090 - accuracy: 0.9253 - val_loss: 0.3168 - val_accuracy: 0.8866\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2040 - accuracy: 0.9264 - val_loss: 0.3010 - val_accuracy: 0.8931\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1991 - accuracy: 0.9295 - val_loss: 0.3013 - val_accuracy: 0.8951\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1951 - accuracy: 0.9299 - val_loss: 0.3122 - val_accuracy: 0.8915\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1914 - accuracy: 0.9319 - val_loss: 0.3092 - val_accuracy: 0.8923\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1867 - accuracy: 0.9334 - val_loss: 0.3042 - val_accuracy: 0.8920\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1823 - accuracy: 0.9347 - val_loss: 0.3133 - val_accuracy: 0.8889\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1784 - accuracy: 0.9372 - val_loss: 0.3215 - val_accuracy: 0.8887\n",
      "313/313 - 0s - loss: 0.3492 - accuracy: 0.8826 - 238ms/epoch - 761us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.9372291564941406\n",
      "Validation accuracy : 0.8886666893959045\n",
      "Loss : 0.34916815161705017\n",
      "Accuracy : 0.8826000094413757\n",
      "\n",
      "Train Accuracy: 0.9372291564941406, Validation Accuracy: 0.8886666893959045\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adam, LR: 0.0001, Batch Size: 64\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.7486 - accuracy: 0.7624 - val_loss: 0.5065 - val_accuracy: 0.8270\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.4700 - accuracy: 0.8408 - val_loss: 0.4528 - val_accuracy: 0.8403\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.4227 - accuracy: 0.8548 - val_loss: 0.4211 - val_accuracy: 0.8524\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.3972 - accuracy: 0.8627 - val_loss: 0.3939 - val_accuracy: 0.8638\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3786 - accuracy: 0.8684 - val_loss: 0.3895 - val_accuracy: 0.8634\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.3633 - accuracy: 0.8721 - val_loss: 0.3837 - val_accuracy: 0.8667\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3496 - accuracy: 0.8780 - val_loss: 0.3677 - val_accuracy: 0.8702\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.3390 - accuracy: 0.8806 - val_loss: 0.3623 - val_accuracy: 0.8718\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3276 - accuracy: 0.8847 - val_loss: 0.3547 - val_accuracy: 0.8771\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.3191 - accuracy: 0.8864 - val_loss: 0.3484 - val_accuracy: 0.8778\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.3120 - accuracy: 0.8882 - val_loss: 0.3466 - val_accuracy: 0.8796\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.3047 - accuracy: 0.8921 - val_loss: 0.3447 - val_accuracy: 0.8783\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.2986 - accuracy: 0.8919 - val_loss: 0.3585 - val_accuracy: 0.8712\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.2917 - accuracy: 0.8962 - val_loss: 0.3362 - val_accuracy: 0.8785\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2844 - accuracy: 0.8975 - val_loss: 0.3332 - val_accuracy: 0.8840\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.2789 - accuracy: 0.8998 - val_loss: 0.3307 - val_accuracy: 0.8844\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2724 - accuracy: 0.9034 - val_loss: 0.3263 - val_accuracy: 0.8838\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2672 - accuracy: 0.9040 - val_loss: 0.3296 - val_accuracy: 0.8838\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.2628 - accuracy: 0.9060 - val_loss: 0.3214 - val_accuracy: 0.8855\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.2586 - accuracy: 0.9070 - val_loss: 0.3165 - val_accuracy: 0.8872\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2522 - accuracy: 0.9096 - val_loss: 0.3176 - val_accuracy: 0.8879\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.2474 - accuracy: 0.9115 - val_loss: 0.3160 - val_accuracy: 0.8876\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.2432 - accuracy: 0.9127 - val_loss: 0.3137 - val_accuracy: 0.8892\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.2392 - accuracy: 0.9146 - val_loss: 0.3280 - val_accuracy: 0.8844\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.2354 - accuracy: 0.9161 - val_loss: 0.3131 - val_accuracy: 0.8890\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.2307 - accuracy: 0.9179 - val_loss: 0.3147 - val_accuracy: 0.8870\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2274 - accuracy: 0.9190 - val_loss: 0.3159 - val_accuracy: 0.8898\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.2238 - accuracy: 0.9201 - val_loss: 0.3208 - val_accuracy: 0.8848\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.2207 - accuracy: 0.9212 - val_loss: 0.3159 - val_accuracy: 0.8885\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2167 - accuracy: 0.9236 - val_loss: 0.3069 - val_accuracy: 0.8903\n",
      "313/313 - 0s - loss: 0.3364 - accuracy: 0.8850 - 181ms/epoch - 579us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.9235625267028809\n",
      "Validation accuracy : 0.890250027179718\n",
      "Loss : 0.33642086386680603\n",
      "Accuracy : 0.8849999904632568\n",
      "\n",
      "Train Accuracy: 0.9235625267028809, Validation Accuracy: 0.890250027179718\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adam, LR: 0.0001, Batch Size: 128\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.9216 - accuracy: 0.7065 - val_loss: 0.5746 - val_accuracy: 0.8116\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5218 - accuracy: 0.8264 - val_loss: 0.4857 - val_accuracy: 0.8331\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4569 - accuracy: 0.8445 - val_loss: 0.4419 - val_accuracy: 0.8462\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4237 - accuracy: 0.8547 - val_loss: 0.4207 - val_accuracy: 0.8534\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4019 - accuracy: 0.8615 - val_loss: 0.4029 - val_accuracy: 0.8597\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3844 - accuracy: 0.8664 - val_loss: 0.3934 - val_accuracy: 0.8598\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3718 - accuracy: 0.8694 - val_loss: 0.3883 - val_accuracy: 0.8608\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3591 - accuracy: 0.8732 - val_loss: 0.3753 - val_accuracy: 0.8658\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3500 - accuracy: 0.8767 - val_loss: 0.3693 - val_accuracy: 0.8676\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3413 - accuracy: 0.8794 - val_loss: 0.3649 - val_accuracy: 0.8696\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3325 - accuracy: 0.8827 - val_loss: 0.3582 - val_accuracy: 0.8717\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3248 - accuracy: 0.8847 - val_loss: 0.3651 - val_accuracy: 0.8697\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3203 - accuracy: 0.8872 - val_loss: 0.3477 - val_accuracy: 0.8770\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3132 - accuracy: 0.8879 - val_loss: 0.3432 - val_accuracy: 0.8791\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3069 - accuracy: 0.8906 - val_loss: 0.3471 - val_accuracy: 0.8769\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3012 - accuracy: 0.8926 - val_loss: 0.3382 - val_accuracy: 0.8832\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2978 - accuracy: 0.8931 - val_loss: 0.3332 - val_accuracy: 0.8821\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2914 - accuracy: 0.8966 - val_loss: 0.3528 - val_accuracy: 0.8726\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2860 - accuracy: 0.8984 - val_loss: 0.3333 - val_accuracy: 0.8840\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2832 - accuracy: 0.8988 - val_loss: 0.3277 - val_accuracy: 0.8823\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2773 - accuracy: 0.9009 - val_loss: 0.3288 - val_accuracy: 0.8828\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2725 - accuracy: 0.9028 - val_loss: 0.3263 - val_accuracy: 0.8833\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2711 - accuracy: 0.9029 - val_loss: 0.3251 - val_accuracy: 0.8842\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2661 - accuracy: 0.9051 - val_loss: 0.3200 - val_accuracy: 0.8857\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2631 - accuracy: 0.9070 - val_loss: 0.3190 - val_accuracy: 0.8859\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2575 - accuracy: 0.9084 - val_loss: 0.3184 - val_accuracy: 0.8866\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2554 - accuracy: 0.9092 - val_loss: 0.3155 - val_accuracy: 0.8868\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2510 - accuracy: 0.9108 - val_loss: 0.3183 - val_accuracy: 0.8850\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2489 - accuracy: 0.9119 - val_loss: 0.3223 - val_accuracy: 0.8856\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2468 - accuracy: 0.9124 - val_loss: 0.3195 - val_accuracy: 0.8871\n",
      "313/313 - 0s - loss: 0.3422 - accuracy: 0.8797 - 182ms/epoch - 581us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adagrad` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adagrad`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adagrad`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.9124374985694885\n",
      "Validation accuracy : 0.8870833516120911\n",
      "Loss : 0.34218573570251465\n",
      "Accuracy : 0.8797000050544739\n",
      "\n",
      "Train Accuracy: 0.9124374985694885, Validation Accuracy: 0.8870833516120911\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adagrad, LR: 0.001, Batch Size: 32\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.1080 - accuracy: 0.6508 - val_loss: 0.7636 - val_accuracy: 0.7508\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7004 - accuracy: 0.7713 - val_loss: 0.6441 - val_accuracy: 0.7866\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6197 - accuracy: 0.7959 - val_loss: 0.5910 - val_accuracy: 0.8068\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5763 - accuracy: 0.8108 - val_loss: 0.5614 - val_accuracy: 0.8120\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5480 - accuracy: 0.8197 - val_loss: 0.5386 - val_accuracy: 0.8187\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5275 - accuracy: 0.8246 - val_loss: 0.5218 - val_accuracy: 0.8257\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5117 - accuracy: 0.8291 - val_loss: 0.5087 - val_accuracy: 0.8271\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4989 - accuracy: 0.8328 - val_loss: 0.4999 - val_accuracy: 0.8282\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4883 - accuracy: 0.8357 - val_loss: 0.4896 - val_accuracy: 0.8327\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4793 - accuracy: 0.8386 - val_loss: 0.4820 - val_accuracy: 0.8347\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4715 - accuracy: 0.8415 - val_loss: 0.4758 - val_accuracy: 0.8357\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4643 - accuracy: 0.8436 - val_loss: 0.4708 - val_accuracy: 0.8386\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4585 - accuracy: 0.8448 - val_loss: 0.4645 - val_accuracy: 0.8411\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4531 - accuracy: 0.8463 - val_loss: 0.4597 - val_accuracy: 0.8424\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4480 - accuracy: 0.8482 - val_loss: 0.4569 - val_accuracy: 0.8420\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4433 - accuracy: 0.8488 - val_loss: 0.4519 - val_accuracy: 0.8448\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4391 - accuracy: 0.8508 - val_loss: 0.4483 - val_accuracy: 0.8458\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4351 - accuracy: 0.8522 - val_loss: 0.4454 - val_accuracy: 0.8464\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4317 - accuracy: 0.8528 - val_loss: 0.4425 - val_accuracy: 0.8468\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4283 - accuracy: 0.8537 - val_loss: 0.4398 - val_accuracy: 0.8483\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4250 - accuracy: 0.8547 - val_loss: 0.4368 - val_accuracy: 0.8492\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4220 - accuracy: 0.8564 - val_loss: 0.4351 - val_accuracy: 0.8495\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4192 - accuracy: 0.8573 - val_loss: 0.4324 - val_accuracy: 0.8500\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4164 - accuracy: 0.8574 - val_loss: 0.4302 - val_accuracy: 0.8518\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4139 - accuracy: 0.8586 - val_loss: 0.4274 - val_accuracy: 0.8515\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4113 - accuracy: 0.8597 - val_loss: 0.4247 - val_accuracy: 0.8539\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4092 - accuracy: 0.8599 - val_loss: 0.4228 - val_accuracy: 0.8532\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4070 - accuracy: 0.8607 - val_loss: 0.4213 - val_accuracy: 0.8540\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4049 - accuracy: 0.8616 - val_loss: 0.4194 - val_accuracy: 0.8547\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4028 - accuracy: 0.8622 - val_loss: 0.4182 - val_accuracy: 0.8550\n",
      "313/313 - 0s - loss: 0.4408 - accuracy: 0.8466 - 239ms/epoch - 762us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adagrad` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adagrad`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adagrad`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8622083067893982\n",
      "Validation accuracy : 0.8550000190734863\n",
      "Loss : 0.4408489763736725\n",
      "Accuracy : 0.8465999960899353\n",
      "\n",
      "Train Accuracy: 0.8622083067893982, Validation Accuracy: 0.8550000190734863\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adagrad, LR: 0.001, Batch Size: 64\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.3718 - accuracy: 0.6010 - val_loss: 0.9537 - val_accuracy: 0.7027\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.8423 - accuracy: 0.7330 - val_loss: 0.7548 - val_accuracy: 0.7583\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.7146 - accuracy: 0.7712 - val_loss: 0.6744 - val_accuracy: 0.7832\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.6511 - accuracy: 0.7930 - val_loss: 0.6272 - val_accuracy: 0.7970\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.6110 - accuracy: 0.8043 - val_loss: 0.5954 - val_accuracy: 0.8052\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.5827 - accuracy: 0.8120 - val_loss: 0.5719 - val_accuracy: 0.8144\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.5613 - accuracy: 0.8194 - val_loss: 0.5537 - val_accuracy: 0.8194\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.5445 - accuracy: 0.8233 - val_loss: 0.5399 - val_accuracy: 0.8217\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.5310 - accuracy: 0.8261 - val_loss: 0.5287 - val_accuracy: 0.8256\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.5194 - accuracy: 0.8291 - val_loss: 0.5195 - val_accuracy: 0.8263\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5096 - accuracy: 0.8327 - val_loss: 0.5129 - val_accuracy: 0.8260\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5012 - accuracy: 0.8341 - val_loss: 0.5027 - val_accuracy: 0.8313\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4938 - accuracy: 0.8364 - val_loss: 0.4964 - val_accuracy: 0.8341\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4873 - accuracy: 0.8380 - val_loss: 0.4912 - val_accuracy: 0.8333\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.4814 - accuracy: 0.8394 - val_loss: 0.4868 - val_accuracy: 0.8337\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.4760 - accuracy: 0.8410 - val_loss: 0.4809 - val_accuracy: 0.8359\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4712 - accuracy: 0.8424 - val_loss: 0.4769 - val_accuracy: 0.8369\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.4667 - accuracy: 0.8439 - val_loss: 0.4732 - val_accuracy: 0.8388\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.4626 - accuracy: 0.8446 - val_loss: 0.4694 - val_accuracy: 0.8397\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4588 - accuracy: 0.8462 - val_loss: 0.4654 - val_accuracy: 0.8420\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4551 - accuracy: 0.8472 - val_loss: 0.4627 - val_accuracy: 0.8421\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4518 - accuracy: 0.8479 - val_loss: 0.4595 - val_accuracy: 0.8428\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4487 - accuracy: 0.8486 - val_loss: 0.4567 - val_accuracy: 0.8442\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.4457 - accuracy: 0.8498 - val_loss: 0.4548 - val_accuracy: 0.8438\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.4428 - accuracy: 0.8503 - val_loss: 0.4523 - val_accuracy: 0.8438\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4401 - accuracy: 0.8512 - val_loss: 0.4495 - val_accuracy: 0.8446\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4376 - accuracy: 0.8516 - val_loss: 0.4477 - val_accuracy: 0.8460\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4352 - accuracy: 0.8528 - val_loss: 0.4459 - val_accuracy: 0.8459\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.4327 - accuracy: 0.8534 - val_loss: 0.4432 - val_accuracy: 0.8472\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.4307 - accuracy: 0.8538 - val_loss: 0.4410 - val_accuracy: 0.8487\n",
      "313/313 - 0s - loss: 0.4627 - accuracy: 0.8393 - 181ms/epoch - 579us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adagrad` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adagrad`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adagrad`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8537708520889282\n",
      "Validation accuracy : 0.8486666679382324\n",
      "Loss : 0.46269291639328003\n",
      "Accuracy : 0.8392999768257141\n",
      "\n",
      "Train Accuracy: 0.8537708520889282, Validation Accuracy: 0.8486666679382324\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adagrad, LR: 0.001, Batch Size: 128\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.6476 - accuracy: 0.5511 - val_loss: 1.1916 - val_accuracy: 0.6688\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.0291 - accuracy: 0.6931 - val_loss: 0.8980 - val_accuracy: 0.7228\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.8444 - accuracy: 0.7360 - val_loss: 0.7846 - val_accuracy: 0.7542\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.7579 - accuracy: 0.7607 - val_loss: 0.7220 - val_accuracy: 0.7708\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.7045 - accuracy: 0.7758 - val_loss: 0.6806 - val_accuracy: 0.7818\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.6671 - accuracy: 0.7871 - val_loss: 0.6481 - val_accuracy: 0.7883\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.6386 - accuracy: 0.7948 - val_loss: 0.6260 - val_accuracy: 0.7973\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.6164 - accuracy: 0.8010 - val_loss: 0.6061 - val_accuracy: 0.8021\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5980 - accuracy: 0.8062 - val_loss: 0.5911 - val_accuracy: 0.8054\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5825 - accuracy: 0.8105 - val_loss: 0.5783 - val_accuracy: 0.8102\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5693 - accuracy: 0.8141 - val_loss: 0.5663 - val_accuracy: 0.8119\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5578 - accuracy: 0.8177 - val_loss: 0.5556 - val_accuracy: 0.8154\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5477 - accuracy: 0.8197 - val_loss: 0.5474 - val_accuracy: 0.8180\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5387 - accuracy: 0.8222 - val_loss: 0.5390 - val_accuracy: 0.8193\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5305 - accuracy: 0.8240 - val_loss: 0.5318 - val_accuracy: 0.8215\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5233 - accuracy: 0.8260 - val_loss: 0.5258 - val_accuracy: 0.8234\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5165 - accuracy: 0.8280 - val_loss: 0.5198 - val_accuracy: 0.8239\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5104 - accuracy: 0.8301 - val_loss: 0.5155 - val_accuracy: 0.8268\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5049 - accuracy: 0.8318 - val_loss: 0.5103 - val_accuracy: 0.8265\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4998 - accuracy: 0.8329 - val_loss: 0.5060 - val_accuracy: 0.8296\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4951 - accuracy: 0.8342 - val_loss: 0.5016 - val_accuracy: 0.8295\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4905 - accuracy: 0.8353 - val_loss: 0.4974 - val_accuracy: 0.8314\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4866 - accuracy: 0.8364 - val_loss: 0.4938 - val_accuracy: 0.8324\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4825 - accuracy: 0.8372 - val_loss: 0.4900 - val_accuracy: 0.8338\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4789 - accuracy: 0.8384 - val_loss: 0.4868 - val_accuracy: 0.8344\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4755 - accuracy: 0.8396 - val_loss: 0.4841 - val_accuracy: 0.8355\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4723 - accuracy: 0.8407 - val_loss: 0.4814 - val_accuracy: 0.8350\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4690 - accuracy: 0.8411 - val_loss: 0.4787 - val_accuracy: 0.8347\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4661 - accuracy: 0.8419 - val_loss: 0.4762 - val_accuracy: 0.8373\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4632 - accuracy: 0.8435 - val_loss: 0.4740 - val_accuracy: 0.8367\n",
      "313/313 - 0s - loss: 0.4954 - accuracy: 0.8280 - 181ms/epoch - 577us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adagrad` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adagrad`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adagrad`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.843500018119812\n",
      "Validation accuracy : 0.8367499709129333\n",
      "Loss : 0.4954470098018646\n",
      "Accuracy : 0.828000009059906\n",
      "\n",
      "Train Accuracy: 0.843500018119812, Validation Accuracy: 0.8367499709129333\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adagrad, LR: 0.0005, Batch Size: 32\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.2905 - accuracy: 0.6480 - val_loss: 0.9124 - val_accuracy: 0.7246\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.8300 - accuracy: 0.7380 - val_loss: 0.7608 - val_accuracy: 0.7590\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7310 - accuracy: 0.7673 - val_loss: 0.6941 - val_accuracy: 0.7780\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6787 - accuracy: 0.7836 - val_loss: 0.6552 - val_accuracy: 0.7872\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6438 - accuracy: 0.7942 - val_loss: 0.6262 - val_accuracy: 0.7943\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6182 - accuracy: 0.8016 - val_loss: 0.6059 - val_accuracy: 0.8000\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5981 - accuracy: 0.8068 - val_loss: 0.5879 - val_accuracy: 0.8048\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5820 - accuracy: 0.8116 - val_loss: 0.5748 - val_accuracy: 0.8067\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5688 - accuracy: 0.8157 - val_loss: 0.5630 - val_accuracy: 0.8107\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5574 - accuracy: 0.8185 - val_loss: 0.5534 - val_accuracy: 0.8134\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5476 - accuracy: 0.8206 - val_loss: 0.5445 - val_accuracy: 0.8155\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5390 - accuracy: 0.8231 - val_loss: 0.5370 - val_accuracy: 0.8183\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5314 - accuracy: 0.8248 - val_loss: 0.5308 - val_accuracy: 0.8205\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5244 - accuracy: 0.8265 - val_loss: 0.5241 - val_accuracy: 0.8215\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5182 - accuracy: 0.8290 - val_loss: 0.5193 - val_accuracy: 0.8231\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5124 - accuracy: 0.8307 - val_loss: 0.5151 - val_accuracy: 0.8250\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5074 - accuracy: 0.8317 - val_loss: 0.5095 - val_accuracy: 0.8255\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5026 - accuracy: 0.8336 - val_loss: 0.5056 - val_accuracy: 0.8270\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4982 - accuracy: 0.8345 - val_loss: 0.5015 - val_accuracy: 0.8279\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4941 - accuracy: 0.8357 - val_loss: 0.4974 - val_accuracy: 0.8284\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4903 - accuracy: 0.8365 - val_loss: 0.4941 - val_accuracy: 0.8308\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4866 - accuracy: 0.8382 - val_loss: 0.4913 - val_accuracy: 0.8317\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4832 - accuracy: 0.8386 - val_loss: 0.4882 - val_accuracy: 0.8328\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4801 - accuracy: 0.8396 - val_loss: 0.4850 - val_accuracy: 0.8328\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4771 - accuracy: 0.8405 - val_loss: 0.4829 - val_accuracy: 0.8341\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4742 - accuracy: 0.8419 - val_loss: 0.4802 - val_accuracy: 0.8355\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4715 - accuracy: 0.8421 - val_loss: 0.4774 - val_accuracy: 0.8361\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4689 - accuracy: 0.8430 - val_loss: 0.4753 - val_accuracy: 0.8369\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4664 - accuracy: 0.8435 - val_loss: 0.4730 - val_accuracy: 0.8374\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4640 - accuracy: 0.8447 - val_loss: 0.4708 - val_accuracy: 0.8386\n",
      "313/313 - 0s - loss: 0.4926 - accuracy: 0.8316 - 235ms/epoch - 751us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adagrad` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adagrad`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adagrad`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8447083234786987\n",
      "Validation accuracy : 0.8385833501815796\n",
      "Loss : 0.4925554096698761\n",
      "Accuracy : 0.83160001039505\n",
      "\n",
      "Train Accuracy: 0.8447083234786987, Validation Accuracy: 0.8385833501815796\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adagrad, LR: 0.0005, Batch Size: 64\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 1.7525 - accuracy: 0.5059 - val_loss: 1.3742 - val_accuracy: 0.6656\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 1.1805 - accuracy: 0.6905 - val_loss: 1.0241 - val_accuracy: 0.7137\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.9498 - accuracy: 0.7216 - val_loss: 0.8767 - val_accuracy: 0.7358\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.8414 - accuracy: 0.7422 - val_loss: 0.7985 - val_accuracy: 0.7521\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.7774 - accuracy: 0.7558 - val_loss: 0.7479 - val_accuracy: 0.7627\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.7337 - accuracy: 0.7673 - val_loss: 0.7115 - val_accuracy: 0.7723\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.7012 - accuracy: 0.7756 - val_loss: 0.6844 - val_accuracy: 0.7800\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.6759 - accuracy: 0.7841 - val_loss: 0.6622 - val_accuracy: 0.7853\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.6551 - accuracy: 0.7909 - val_loss: 0.6444 - val_accuracy: 0.7906\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.6374 - accuracy: 0.7962 - val_loss: 0.6281 - val_accuracy: 0.7963\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.6223 - accuracy: 0.7998 - val_loss: 0.6150 - val_accuracy: 0.8004\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.6093 - accuracy: 0.8027 - val_loss: 0.6041 - val_accuracy: 0.8027\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.5979 - accuracy: 0.8073 - val_loss: 0.5933 - val_accuracy: 0.8054\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.5878 - accuracy: 0.8095 - val_loss: 0.5838 - val_accuracy: 0.8068\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.5787 - accuracy: 0.8126 - val_loss: 0.5763 - val_accuracy: 0.8088\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.5703 - accuracy: 0.8149 - val_loss: 0.5681 - val_accuracy: 0.8122\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.5627 - accuracy: 0.8171 - val_loss: 0.5615 - val_accuracy: 0.8136\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.5557 - accuracy: 0.8188 - val_loss: 0.5547 - val_accuracy: 0.8145\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.5491 - accuracy: 0.8205 - val_loss: 0.5492 - val_accuracy: 0.8170\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.5431 - accuracy: 0.8223 - val_loss: 0.5437 - val_accuracy: 0.8189\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.5375 - accuracy: 0.8239 - val_loss: 0.5383 - val_accuracy: 0.8201\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.5323 - accuracy: 0.8252 - val_loss: 0.5336 - val_accuracy: 0.8204\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.5275 - accuracy: 0.8263 - val_loss: 0.5295 - val_accuracy: 0.8219\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5230 - accuracy: 0.8279 - val_loss: 0.5256 - val_accuracy: 0.8227\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5186 - accuracy: 0.8289 - val_loss: 0.5223 - val_accuracy: 0.8243\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5147 - accuracy: 0.8303 - val_loss: 0.5180 - val_accuracy: 0.8267\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5108 - accuracy: 0.8311 - val_loss: 0.5149 - val_accuracy: 0.8275\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5072 - accuracy: 0.8329 - val_loss: 0.5115 - val_accuracy: 0.8278\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.5038 - accuracy: 0.8337 - val_loss: 0.5081 - val_accuracy: 0.8294\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.5006 - accuracy: 0.8347 - val_loss: 0.5054 - val_accuracy: 0.8293\n",
      "313/313 - 0s - loss: 0.5257 - accuracy: 0.8211 - 179ms/epoch - 573us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adagrad` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adagrad`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adagrad`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8347499966621399\n",
      "Validation accuracy : 0.8293333053588867\n",
      "Loss : 0.5257022380828857\n",
      "Accuracy : 0.8210999965667725\n",
      "\n",
      "Train Accuracy: 0.8347499966621399, Validation Accuracy: 0.8293333053588867\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adagrad, LR: 0.0005, Batch Size: 128\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.8790 - accuracy: 0.4060 - val_loss: 1.5689 - val_accuracy: 0.6123\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.3842 - accuracy: 0.6474 - val_loss: 1.2224 - val_accuracy: 0.6722\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.1326 - accuracy: 0.6827 - val_loss: 1.0404 - val_accuracy: 0.6999\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.9928 - accuracy: 0.7052 - val_loss: 0.9345 - val_accuracy: 0.7195\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.9067 - accuracy: 0.7226 - val_loss: 0.8656 - val_accuracy: 0.7331\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.8482 - accuracy: 0.7371 - val_loss: 0.8176 - val_accuracy: 0.7487\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.8052 - accuracy: 0.7496 - val_loss: 0.7806 - val_accuracy: 0.7581\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.7717 - accuracy: 0.7594 - val_loss: 0.7513 - val_accuracy: 0.7640\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.7445 - accuracy: 0.7673 - val_loss: 0.7270 - val_accuracy: 0.7695\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.7217 - accuracy: 0.7742 - val_loss: 0.7072 - val_accuracy: 0.7763\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.7022 - accuracy: 0.7800 - val_loss: 0.6896 - val_accuracy: 0.7804\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.6854 - accuracy: 0.7854 - val_loss: 0.6746 - val_accuracy: 0.7865\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.6706 - accuracy: 0.7905 - val_loss: 0.6614 - val_accuracy: 0.7883\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.6575 - accuracy: 0.7930 - val_loss: 0.6492 - val_accuracy: 0.7937\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.6455 - accuracy: 0.7969 - val_loss: 0.6386 - val_accuracy: 0.7964\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.6350 - accuracy: 0.7991 - val_loss: 0.6289 - val_accuracy: 0.7983\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.6253 - accuracy: 0.8015 - val_loss: 0.6201 - val_accuracy: 0.8015\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.6165 - accuracy: 0.8036 - val_loss: 0.6122 - val_accuracy: 0.8023\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.6085 - accuracy: 0.8060 - val_loss: 0.6056 - val_accuracy: 0.8030\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.6009 - accuracy: 0.8084 - val_loss: 0.5981 - val_accuracy: 0.8048\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5941 - accuracy: 0.8098 - val_loss: 0.5911 - val_accuracy: 0.8073\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5877 - accuracy: 0.8115 - val_loss: 0.5855 - val_accuracy: 0.8092\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5818 - accuracy: 0.8130 - val_loss: 0.5800 - val_accuracy: 0.8108\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5761 - accuracy: 0.8149 - val_loss: 0.5750 - val_accuracy: 0.8111\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5708 - accuracy: 0.8167 - val_loss: 0.5711 - val_accuracy: 0.8102\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5659 - accuracy: 0.8175 - val_loss: 0.5653 - val_accuracy: 0.8145\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5611 - accuracy: 0.8186 - val_loss: 0.5613 - val_accuracy: 0.8148\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5566 - accuracy: 0.8197 - val_loss: 0.5571 - val_accuracy: 0.8175\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5524 - accuracy: 0.8207 - val_loss: 0.5530 - val_accuracy: 0.8176\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5484 - accuracy: 0.8222 - val_loss: 0.5494 - val_accuracy: 0.8198\n",
      "313/313 - 0s - loss: 0.5704 - accuracy: 0.8104 - 182ms/epoch - 582us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adagrad` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adagrad`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adagrad`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8221874833106995\n",
      "Validation accuracy : 0.8197500109672546\n",
      "Loss : 0.5704092979431152\n",
      "Accuracy : 0.8104000091552734\n",
      "\n",
      "Train Accuracy: 0.8221874833106995, Validation Accuracy: 0.8197500109672546\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adagrad, LR: 0.0001, Batch Size: 32\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 2.0676 - accuracy: 0.3203 - val_loss: 1.8610 - val_accuracy: 0.4948\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.7239 - accuracy: 0.5326 - val_loss: 1.5955 - val_accuracy: 0.5836\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.5057 - accuracy: 0.6175 - val_loss: 1.4134 - val_accuracy: 0.6501\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.3543 - accuracy: 0.6536 - val_loss: 1.2857 - val_accuracy: 0.6683\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.2460 - accuracy: 0.6660 - val_loss: 1.1925 - val_accuracy: 0.6773\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.1655 - accuracy: 0.6735 - val_loss: 1.1223 - val_accuracy: 0.6867\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.1029 - accuracy: 0.6802 - val_loss: 1.0659 - val_accuracy: 0.6933\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.0527 - accuracy: 0.6864 - val_loss: 1.0212 - val_accuracy: 0.6988\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.0121 - accuracy: 0.6930 - val_loss: 0.9842 - val_accuracy: 0.7056\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.9781 - accuracy: 0.6991 - val_loss: 0.9531 - val_accuracy: 0.7109\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.9492 - accuracy: 0.7048 - val_loss: 0.9263 - val_accuracy: 0.7168\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.9240 - accuracy: 0.7112 - val_loss: 0.9030 - val_accuracy: 0.7237\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.9020 - accuracy: 0.7174 - val_loss: 0.8822 - val_accuracy: 0.7282\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.8826 - accuracy: 0.7223 - val_loss: 0.8643 - val_accuracy: 0.7341\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.8653 - accuracy: 0.7283 - val_loss: 0.8482 - val_accuracy: 0.7387\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.8499 - accuracy: 0.7318 - val_loss: 0.8336 - val_accuracy: 0.7431\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.8359 - accuracy: 0.7360 - val_loss: 0.8205 - val_accuracy: 0.7462\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.8233 - accuracy: 0.7395 - val_loss: 0.8087 - val_accuracy: 0.7499\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.8117 - accuracy: 0.7439 - val_loss: 0.7977 - val_accuracy: 0.7538\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.8011 - accuracy: 0.7470 - val_loss: 0.7876 - val_accuracy: 0.7581\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7912 - accuracy: 0.7506 - val_loss: 0.7782 - val_accuracy: 0.7588\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7820 - accuracy: 0.7527 - val_loss: 0.7697 - val_accuracy: 0.7633\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7735 - accuracy: 0.7564 - val_loss: 0.7615 - val_accuracy: 0.7658\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7655 - accuracy: 0.7586 - val_loss: 0.7540 - val_accuracy: 0.7679\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7580 - accuracy: 0.7603 - val_loss: 0.7469 - val_accuracy: 0.7706\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7509 - accuracy: 0.7627 - val_loss: 0.7403 - val_accuracy: 0.7722\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7443 - accuracy: 0.7651 - val_loss: 0.7339 - val_accuracy: 0.7739\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7380 - accuracy: 0.7664 - val_loss: 0.7281 - val_accuracy: 0.7747\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7320 - accuracy: 0.7682 - val_loss: 0.7224 - val_accuracy: 0.7757\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7263 - accuracy: 0.7699 - val_loss: 0.7170 - val_accuracy: 0.7770\n",
      "313/313 - 0s - loss: 0.7407 - accuracy: 0.7625 - 237ms/epoch - 758us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adagrad` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adagrad`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adagrad`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.7699166536331177\n",
      "Validation accuracy : 0.7770000100135803\n",
      "Loss : 0.7406971454620361\n",
      "Accuracy : 0.762499988079071\n",
      "\n",
      "Train Accuracy: 0.7699166536331177, Validation Accuracy: 0.7770000100135803\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adagrad, LR: 0.0001, Batch Size: 64\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 2.1846 - accuracy: 0.2117 - val_loss: 2.0055 - val_accuracy: 0.3618\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.8947 - accuracy: 0.4782 - val_loss: 1.7840 - val_accuracy: 0.5767\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.7008 - accuracy: 0.5974 - val_loss: 1.6146 - val_accuracy: 0.6191\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 1.5525 - accuracy: 0.6261 - val_loss: 1.4823 - val_accuracy: 0.6417\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 1.4360 - accuracy: 0.6450 - val_loss: 1.3780 - val_accuracy: 0.6634\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 1.3430 - accuracy: 0.6629 - val_loss: 1.2937 - val_accuracy: 0.6754\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 1.2669 - accuracy: 0.6725 - val_loss: 1.2237 - val_accuracy: 0.6848\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 1.2032 - accuracy: 0.6814 - val_loss: 1.1651 - val_accuracy: 0.6914\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 1.1501 - accuracy: 0.6874 - val_loss: 1.1163 - val_accuracy: 0.6975\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.1055 - accuracy: 0.6930 - val_loss: 1.0752 - val_accuracy: 0.7034\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 1.0675 - accuracy: 0.6989 - val_loss: 1.0399 - val_accuracy: 0.7074\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 1.0348 - accuracy: 0.7043 - val_loss: 1.0096 - val_accuracy: 0.7125\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 1.0063 - accuracy: 0.7101 - val_loss: 0.9828 - val_accuracy: 0.7164\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.9813 - accuracy: 0.7140 - val_loss: 0.9592 - val_accuracy: 0.7191\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.9590 - accuracy: 0.7181 - val_loss: 0.9383 - val_accuracy: 0.7245\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.9391 - accuracy: 0.7218 - val_loss: 0.9194 - val_accuracy: 0.7275\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.9210 - accuracy: 0.7249 - val_loss: 0.9024 - val_accuracy: 0.7314\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.9046 - accuracy: 0.7283 - val_loss: 0.8869 - val_accuracy: 0.7341\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.8896 - accuracy: 0.7313 - val_loss: 0.8725 - val_accuracy: 0.7376\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.8757 - accuracy: 0.7337 - val_loss: 0.8593 - val_accuracy: 0.7397\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.8629 - accuracy: 0.7368 - val_loss: 0.8471 - val_accuracy: 0.7420\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.8510 - accuracy: 0.7392 - val_loss: 0.8358 - val_accuracy: 0.7451\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.8399 - accuracy: 0.7421 - val_loss: 0.8253 - val_accuracy: 0.7465\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.8296 - accuracy: 0.7452 - val_loss: 0.8155 - val_accuracy: 0.7485\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.8199 - accuracy: 0.7481 - val_loss: 0.8061 - val_accuracy: 0.7497\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.8108 - accuracy: 0.7499 - val_loss: 0.7975 - val_accuracy: 0.7522\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.8022 - accuracy: 0.7518 - val_loss: 0.7893 - val_accuracy: 0.7548\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.7941 - accuracy: 0.7542 - val_loss: 0.7815 - val_accuracy: 0.7566\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.7865 - accuracy: 0.7558 - val_loss: 0.7742 - val_accuracy: 0.7580\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.7792 - accuracy: 0.7583 - val_loss: 0.7673 - val_accuracy: 0.7598\n",
      "313/313 - 0s - loss: 0.7906 - accuracy: 0.7522 - 187ms/epoch - 597us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adagrad` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adagrad`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adagrad`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.7583333253860474\n",
      "Validation accuracy : 0.7598333358764648\n",
      "Loss : 0.7905558347702026\n",
      "Accuracy : 0.7522000074386597\n",
      "\n",
      "Train Accuracy: 0.7583333253860474, Validation Accuracy: 0.7598333358764648\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adagrad, LR: 0.0001, Batch Size: 128\n",
      "Training with model model_v1 ...\n",
      "Epoch 1/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 2.2350 - accuracy: 0.1635 - val_loss: 2.1173 - val_accuracy: 0.2829\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 2.0332 - accuracy: 0.3860 - val_loss: 1.9509 - val_accuracy: 0.4608\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.8902 - accuracy: 0.4903 - val_loss: 1.8242 - val_accuracy: 0.5300\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.7766 - accuracy: 0.5437 - val_loss: 1.7191 - val_accuracy: 0.5706\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.6794 - accuracy: 0.5816 - val_loss: 1.6272 - val_accuracy: 0.6033\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.5942 - accuracy: 0.6101 - val_loss: 1.5468 - val_accuracy: 0.6237\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.5198 - accuracy: 0.6271 - val_loss: 1.4766 - val_accuracy: 0.6363\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.4543 - accuracy: 0.6378 - val_loss: 1.4141 - val_accuracy: 0.6450\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.3958 - accuracy: 0.6452 - val_loss: 1.3581 - val_accuracy: 0.6524\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.3435 - accuracy: 0.6510 - val_loss: 1.3084 - val_accuracy: 0.6597\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.2971 - accuracy: 0.6559 - val_loss: 1.2643 - val_accuracy: 0.6647\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.2558 - accuracy: 0.6598 - val_loss: 1.2248 - val_accuracy: 0.6685\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.2186 - accuracy: 0.6648 - val_loss: 1.1895 - val_accuracy: 0.6739\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.1851 - accuracy: 0.6698 - val_loss: 1.1575 - val_accuracy: 0.6777\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.1548 - accuracy: 0.6743 - val_loss: 1.1287 - val_accuracy: 0.6826\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.1273 - accuracy: 0.6792 - val_loss: 1.1026 - val_accuracy: 0.6875\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.1024 - accuracy: 0.6840 - val_loss: 1.0788 - val_accuracy: 0.6915\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.0796 - accuracy: 0.6878 - val_loss: 1.0571 - val_accuracy: 0.6961\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.0588 - accuracy: 0.6920 - val_loss: 1.0373 - val_accuracy: 0.6992\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.0395 - accuracy: 0.6961 - val_loss: 1.0189 - val_accuracy: 0.7032\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.0217 - accuracy: 0.7000 - val_loss: 1.0019 - val_accuracy: 0.7078\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.0052 - accuracy: 0.7038 - val_loss: 0.9861 - val_accuracy: 0.7107\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.9898 - accuracy: 0.7076 - val_loss: 0.9714 - val_accuracy: 0.7138\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.9754 - accuracy: 0.7108 - val_loss: 0.9576 - val_accuracy: 0.7164\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.9620 - accuracy: 0.7142 - val_loss: 0.9447 - val_accuracy: 0.7188\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.9494 - accuracy: 0.7171 - val_loss: 0.9327 - val_accuracy: 0.7223\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.9375 - accuracy: 0.7198 - val_loss: 0.9213 - val_accuracy: 0.7249\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.9263 - accuracy: 0.7232 - val_loss: 0.9106 - val_accuracy: 0.7272\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.9157 - accuracy: 0.7254 - val_loss: 0.9005 - val_accuracy: 0.7297\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.9057 - accuracy: 0.7283 - val_loss: 0.8909 - val_accuracy: 0.7319\n",
      "313/313 - 0s - loss: 0.9128 - accuracy: 0.7182 - 179ms/epoch - 571us/step\n",
      "\n",
      "Training accuracy : 0.7283124923706055\n",
      "Validation accuracy : 0.7319166660308838\n",
      "Loss : 0.9127663969993591\n",
      "Accuracy : 0.7182000279426575\n",
      "\n",
      "Train Accuracy: 0.7283124923706055, Validation Accuracy: 0.7319166660308838\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_accuracy = 0\n",
    "best_params = {}\n",
    "best_history = {}  # To store the history of the best model\n",
    "\n",
    "for opt_name, opt_class in optimizers.items():\n",
    "    for lr in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"\\nTraining with {opt_name}, LR: {lr}, Batch Size: {batch_size}\")\n",
    "            model = fmm_module.FMM.get_model()\n",
    "            optimizer = opt_class(learning_rate=lr)\n",
    "            history = fmm_module.FMM.compile_and_train(\n",
    "                model, X_train, y_train, optimizer, batch_size=batch_size\n",
    "            )\n",
    "            \n",
    "\n",
    "            loss, accuracy, train_accuracy, val_accuracy = fmm_module.FMM.evaluate(model, X_test, y_test, history)\n",
    "            # Update best parameters if current model is better\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                best_params = {\n",
    "                    'optimizer': opt_name,\n",
    "                    'learning_rate': lr,\n",
    "                    'batch_size': batch_size,\n",
    "                    'train_accuracy': train_accuracy,\n",
    "                    'val_accuracy': val_accuracy\n",
    "                }\n",
    "                best_history = history  # Save the best history for plotting\n",
    "\n",
    "            print(f\"Train Accuracy: {train_accuracy}, Validation Accuracy: {val_accuracy}\")\n",
    "            print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d2e10e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Best Hyperparameters:\n",
      "Optimizer: Adam\n",
      "Learning Rate: 0.0005\n",
      "Batch Size: 32\n",
      "Training Accuracy: 0.9582499861717224\n",
      "Validation Accuracy: 0.8955000042915344\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Best Hyperparameters:\")\n",
    "print(f\"Optimizer: {best_params['optimizer']}\")\n",
    "print(f\"Learning Rate: {best_params['learning_rate']}\")\n",
    "print(f\"Batch Size: {best_params['batch_size']}\")\n",
    "print(f\"Training Accuracy: {best_params['train_accuracy']}\")\n",
    "print(f\"Validation Accuracy: {best_params['val_accuracy']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a12815d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(best_history.history['loss'], label='Training Loss')\n",
    "plt.plot(best_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title(f'Loss - Best Model\\nOptimizer: {best_params[\"optimizer\"]}, LR: {best_params[\"learning_rate\"]}, Batch: {best_params[\"batch_size\"]}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(best_history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(best_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title(f'Accuracy - Best Model\\nOptimizer: {best_params[\"optimizer\"]}, LR: {best_params[\"learning_rate\"]}, Batch: {best_params[\"batch_size\"]}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ee8e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
